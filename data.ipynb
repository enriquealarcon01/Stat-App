{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Récupération et formatage des données**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a class=\"anchor\" id=\"partie1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook contient les codes nécessaires à la récupération et au formatage des données. Les données ont été récupéré sur le site de [l'OCDE](https://data-explorer.oecd.org/) et sur celui [d'Eurostat](https://ec.europa.eu/eurostat/fr/data/database). \n",
    "\n",
    "Nous récupérons les données en csv et les transformons en un unique csv contenant pour chacun des pays les valeurs trimestrielles (de Q1 en 1995 à Q3 de 2024 <span style=\"color:red ; font-size:20px;\"> faire attention les working hours ne vont qu'au Q3 de 2024 </span>) de PIB, d'heure de travail, des taux d'intérêts à court et long terme et de l'indice à la consommation des prix. Pour l'instant il nous manque aussi les anticipations des taux et les rendements des marchés financiers. Le csv final de ce notebook servira de base de données par la suite. \n",
    "\n",
    "Les pays retenus sont ceux pour lesquels on a réussi à collecter toutes les données relatives aux variables d'intérêt: Autriche (Austria) Belgique (Belgium) Bulgarie (Bulgaria) Tchéquie (Czechia) Danemark (Denmark) Estonie (Estonia) Finlande (Finland) France (France) Allemagne (Germany) Grèce (Greece) Hongrie (Hungary) Islande (Iceland) Irlande (Ireland) Italie (Italy) Lettonie (Latvia) Lituanie (Lithuania) Luxembourg (Luxembourg) Pays-Bas (Netherlands) Norvège (Norway) Pologne (Poland) Portugal (Portugal) Roumanie (Romania) Slovaquie (Slovakia) Slovénie (Slovenia) Espagne (Spain) Suède (Sweden) Suisse (Switzerland) Royaume-Uni (United Kingdom).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "* [Introduction](#partie1)\n",
    "* [Récupération des données](#partie2)\n",
    "    * [Données de PIB](#partie21)\n",
    "    * [Données d'heures de travail](#partie22)\n",
    "    * [Données des taux d'intérêts](#partie23)\n",
    "    * [Données d'indice à la consommation des prix](#partie24)\n",
    "    * [Données de croissance potentielle](#partie25)\n",
    "* [Création de la base de données](#partie3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données <a class=\"anchor\" id=\"partie2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.12/site-packages (from openpyxl->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons la liste qui contient les pays cités précédemment, à noter que la Slovaquie apparaît sous le nom Slovakia pour Eurostat et Slovak Republic pour l'OCDE, elle est donc sous deux noms dans notre liste de pays à sélectionner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste comportant les pays choisis\n",
    "countries_selected=[\n",
    "    \"Austria\", \"Belgium\", \"Bulgaria\", \"Czechia\", \"Czech Republic\", \"Denmark\", \"Estonia\", \n",
    "    \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \"Iceland\", \n",
    "    \"Ireland\", \"Italy\", \"Latvia\", \"Lithuania\", \"Luxembourg\", \"Netherlands\", \n",
    "    \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Slovakia\", \"Slovak Republic\",\n",
    "    \"Slovenia\", \"Spain\", \"Sweden\", \"Switzerland\", \"United Kingdom\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données de PIB <a class=\"anchor\" id=\"partie21\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous extrayons les données relatives au PIB depuis Eurostat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données \n",
    "df_GDP = pd.read_csv(\"Données_extraites/GDP_trimestriel_eurostat.csv\", encoding='utf-8')\n",
    "\n",
    "# Sélection des colonnes nécessaires avec les pays sélectionnés\n",
    "df_GDP_selected = df_GDP[df_GDP['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Modification du format pour avoir les périodes en index et les noms des pays en nom de colonnes\n",
    "df_GDP = df_GDP_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Transformation des colonnes en PIB_Nom_du_pays\n",
    "df_GDP.columns = [f'PIB_{col}' for col in df_GDP.columns]\n",
    "df_GDP.columns = [col.replace(\" \", \"_\") for col in df_GDP.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données de d'heure de travail <a class=\"anchor\" id=\"partie22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous extrayons les données relatives aux heures de travail depuis Eurostat. Nous avons trois base de données pour les heures de travail. Une base annuelle de 1995 à 1997, et deux bases trimestrielles allant du Q1 de 1998 au Q4 de 2007 et du Q1 de 2008 au Q3 de 2024.\n",
    "\n",
    "Nous voulons traiter les données dans l'ordre chronologique. Nous devons donc pour la première période récupérer les données annuelles et les transformer en trimestrielles, nous choisissons d'interpoler les données manquantes de manière linéaire en donnant la valeur annuelle au Q1. Pour cela nous avons besoin de traiter les données de 1998 à 2007 avant afin de pouvoir faire l'interpolation entre le Q1 de 1997 et celui de 1998.\n",
    "\n",
    "Nous récupérons donc d'abord toutes les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données de 1995 à 1997 \n",
    "df_working_hours_95_97 = pd.read_csv(\"Données_extraites/Working_hours_eurostat_1995-1997.csv\", encoding='utf-8')\n",
    "\n",
    "# Sélection des colonnes nécessaires avec les pays sélectionnés\n",
    "df_working_hours_95_97_selected = df_working_hours_95_97[df_working_hours_95_97['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Modification du format pour avoir les périodes en index et les noms des pays en nom de colonnes\n",
    "df_working_hours_95_97_pivot = df_working_hours_95_97_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Transformation des colonnes en WH_Nom_du_pays\n",
    "df_working_hours_95_97_pivot.columns = [f'WH_{col}' for col in df_working_hours_95_97_pivot.columns]\n",
    "df_working_hours_95_97_pivot.columns = [col.replace(\" \", \"_\") for col in df_working_hours_95_97_pivot.columns]\n",
    "\n",
    "# Même processus pour celles de 1998 à 2007 et celles de 2008 à 2024\n",
    "df_working_hours_98_07 = pd.read_csv(\"Données_extraites/Working_hours_eurostat_1998-2007.csv\", encoding='utf-8')\n",
    "df_working_hours_98_07_selected = df_working_hours_98_07[df_working_hours_98_07['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "df_working_hours_98_07_pivot = df_working_hours_98_07_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "df_working_hours_98_07_pivot.columns = [f'WH_{col}' for col in df_working_hours_98_07_pivot.columns]\n",
    "df_working_hours_98_07_pivot.columns = [col.replace(\" \", \"_\") for col in df_working_hours_98_07_pivot.columns]\n",
    "\n",
    "df_working_hours_08_24 = pd.read_csv(\"Données_extraites/Working_hours_eurostat_2008-2024.csv\", encoding='utf-8')\n",
    "df_working_hours_08_24_selected = df_working_hours_08_24[df_working_hours_08_24['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "df_working_hours_08_24_pivot = df_working_hours_08_24_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "df_working_hours_08_24_pivot.columns = [f'WH_{col}' for col in df_working_hours_08_24_pivot.columns]\n",
    "df_working_hours_08_24_pivot.columns = [col.replace(\" \", \"_\") for col in df_working_hours_08_24_pivot.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant nous configurons l'interpolation. Pour clarifier le point précédent, l'interpolation est linéaire, donc va tracer une droite entre deux périodes avec une valeur présente et inférer une valeur aux périodes pour laquelle la valeur est manquante et qui se trouvent entre nos deux périodes. Comme nous faisons le choix de donner la valeur annuelle au Q1 il faut ajouter une valeur de 1998 pour pouvoir inférer les valeurs du Q2, Q3 et Q4 de 1997, ce que nous faisons donc en ajoutant le Q1 de 1998 à notre table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du Q1 de 1998 et suppresion de la table pour éviter les doublons lors de la concaténation\n",
    "Q1_1998 =  df_working_hours_98_07_pivot.loc[['1998-Q1']]\n",
    "df_working_hours_98_07_pivot = df_working_hours_98_07_pivot.drop(index='1998-Q1', errors='ignore')\n",
    "\n",
    "# Ajout des trimestres vierges aux données de 1995 à 1998 pour ensuite les compléter par interpolation\n",
    "new_index = []\n",
    "values = []\n",
    "for year in df_working_hours_95_97_pivot.index:\n",
    "    # Ajout du Q1 pour chaque année (avec la valeur annuelle)\n",
    "    new_index.append(f\"{year}-Q1\")\n",
    "    values.append(df_working_hours_95_97_pivot.loc[year].values)\n",
    "    \n",
    "    # Ajouts des Q2, Q3 et Q4 (lignes vides pour l'instant)\n",
    "    for quarter in ['Q2', 'Q3', 'Q4']:\n",
    "        new_index.append(f\"{year}-{quarter}\")\n",
    "        values.append([None] * len(df_working_hours_95_97_pivot.columns))\n",
    "\n",
    "# Création du dataframe avec les lignes vides\n",
    "df_working_hours_95_97_pivot = pd.DataFrame(values, index=new_index, columns=df_working_hours_95_97_pivot.columns)\n",
    "df_working_hours_95_97_pivot.index.name = 'TIME_PERIOD'\n",
    "\n",
    "df_working_hours_95_97_pivot = pd.concat([df_working_hours_95_97_pivot, Q1_1998])\n",
    "\n",
    "# Interpolation\n",
    "df_working_hours_95_97_pivot = df_working_hours_95_97_pivot.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous procédons à la concaténation des données pour avoir un unique dataframe contenant les heures de travail trimestrielles de 1995 à 2024. Seulement nous devons supprimer les index de 2008 dans le dataframe de 1998 à 2007 car sinon ils seraient en double lors de la concaténation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppresion des index\n",
    "df_working_hours_98_07_pivot = df_working_hours_98_07_pivot.drop(index = ['2008-Q1','2008-Q2', '2008-Q3','2008-Q4'] , errors='ignore')\n",
    "\n",
    "# Concaténation\n",
    "df_working_hours = pd.concat(\n",
    "    [df_working_hours_95_97_pivot, df_working_hours_98_07_pivot, df_working_hours_08_24_pivot], \n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données des taux d'intérêts <a class=\"anchor\" id=\"partie23\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les taux d'intérêts nous récupérons des taux de long-terme, à savoir des obligations d'Etat à 10 ans, et des taux de court-terme. Les taux à court terme sont généralement soit le taux interbancaire à trois mois lié aux prêts accordés et acceptés par les banques pour tout excès ou manque de liquidité sur plusieurs mois, soit le taux associé aux bons du Trésor, aux certificats de dépôt ou à des instruments comparables, chacun d'une durée de trois mois. Pour les pays de la zone euro, le « European Interbank Offered Rate » à trois mois est utilisé à partir de la date à laquelle le pays a rejoint l'euro.  \n",
    "\n",
    "Nous commençons par récupérer les taux de long-terme puis ceux de court-terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "df_LT_IR = pd.read_csv('Données_extraites/Long_term_IR_OCDE.csv', encoding='utf-8')\n",
    "\n",
    "# Sélection des colonnes nécessaires avec les pays sélectionnés\n",
    "df_LT_IR_selected = df_LT_IR[df_LT_IR['Reference area'].isin(countries_selected)][['Reference area', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Modification du format pour avoir les périodes en index et les noms des pays en nom de colonnes\n",
    "df_LT_IR = df_LT_IR_selected.pivot_table(index='TIME_PERIOD', columns='Reference area', values='OBS_VALUE')\n",
    "\n",
    "# Modification des colonnes pour inclure le préfixe LT_IR_ (=Long Term Interest Rate)\n",
    "df_LT_IR.columns = [f'LT_IR_{col}' for col in df_LT_IR.columns]\n",
    "df_LT_IR.columns = [col.replace(\" \", \"_\") for col in df_LT_IR.columns]\n",
    "df_LT_IR = df_LT_IR.rename(columns={'LT_IR_Slovak_Republic': 'LT_IR_Slovakia'})\n",
    "\n",
    "# Même processus pour les taux de long-terme\n",
    "df_ST_IR=pd.read_csv('Données_extraites/Short_term_IR_OCDE.csv', encoding='utf-8')\n",
    "df_ST_IR_selected = df_ST_IR[df_ST_IR['Reference area'].isin(countries_selected)][['Reference area', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "df_ST_IR = df_ST_IR_selected.pivot_table(index='TIME_PERIOD', columns='Reference area', values='OBS_VALUE')\n",
    "df_ST_IR.columns = [f'ST_IR_{col}' for col in df_ST_IR.columns]\n",
    "df_ST_IR.columns = [col.replace(\" \", \"_\") for col in df_ST_IR.columns]\n",
    "df_ST_IR = df_ST_IR.rename(columns={'ST_IR_Slovak_Republic': 'ST_IR_Slovakia'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données d'indice à la consommation des prix <a class=\"anchor\" id=\"partie24\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous récupérons les données relatives à l'indice à la consommation des prix, qui nécessite des modifications car les données actuelles sont mensuelles et non trimestrielles. Nous faisons le choix de les transformer en trimestrielles en attribuant les moyennes mensuelles sur trois mois à notre valeur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "df_CPI=pd.read_csv('Données_extraites/CPI_annual_eurostat_1996-2024.csv', encoding='utf-8')\n",
    "\n",
    "# Sélection des colonnes nécessaires avec les pays sélectionnés\n",
    "df_CPI_selected = df_CPI[df_CPI['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Modification du format pour avoir les périodes en index et les noms des pays en nom de colonnes\n",
    "df_CPI = df_CPI_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Transformation des colonnes en CPI_Nom_du_pays\n",
    "df_CPI.columns = [f'CPI_{col}' for col in df_CPI.columns]\n",
    "df_CPI.columns = [col.replace(\" \", \"_\") for col in df_CPI.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous transformons ici les données mensuelles en trimestrielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des mois en trimestre\n",
    "df_CPI.index = pd.PeriodIndex(df_CPI.index, freq='Q')\n",
    "\n",
    "# Pour chaque trimestre on a donc 3 valeurs on fait donc la moyenne\n",
    "df_CPI = df_CPI.groupby(df_CPI.index).mean()\n",
    "\n",
    "# Modification du format de l'index qui est YYYYQQ en YYYY-QQ pour l'accorder aux autres tables\n",
    "df_CPI.index = df_CPI.index.astype(str).str.replace(r'(\\d{4})Q(\\d)', r'\\1-Q\\2', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données de croissance potentielle <a class=\"anchor\" id=\"partie25\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous récupérons les données relatives à l'étude de la croissance potentielle sur un dataframe de la banque mondiale. Il nous faut faire attenion aux noms donnés à la Tchéquie et la Slovaquie qui sont Czech Republic et Slovak Republic et non Czechia et Slovakia comme nous le nommons dans notre nomenclature.\n",
    "\n",
    "De plus les données sont annuelles, donc pour les rendre trimestrielles nous les divisons par 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données \n",
    "df_potential_growth= pd.read_excel(\"Données_extraites/potential_growth_database.xlsx\",sheet_name=\"PotentialGrowthMeasures\")\n",
    "\n",
    "# Modification des colonnes (pour garder une homogénéité entre les différents df)\n",
    "df_potential_growth= df_potential_growth.rename(columns={'Country':'geo', 'Year':'TIME_PERIOD','HP: Potential real GDP growth (percent)':'OBS_VALUE'})\n",
    "df_potential_growth = df_potential_growth[df_potential_growth['TIME_PERIOD'] >= 1995]\n",
    "\n",
    "# Choix des colonnes nécessaires avec les pays sélectionnés. \n",
    "df_potential_growth_selected = df_potential_growth[df_potential_growth['geo'].isin(countries_selected)][['geo', 'TIME_PERIOD', 'OBS_VALUE']]\n",
    "\n",
    "# Modification du format pour avoir les périodes en index et les noms des pays en nom de colonnes\n",
    "df_potential_growth= df_potential_growth_selected.pivot_table(index='TIME_PERIOD', columns='geo', values='OBS_VALUE')\n",
    "\n",
    "# Transformation des colonnes en P_Growth_pays\n",
    "df_potential_growth = df_potential_growth.rename(columns={'Czech Republic': 'Czechia'})\n",
    "df_potential_growth = df_potential_growth.rename(columns={'Slovak Republic': 'Slovakia'})\n",
    "df_potential_growth.columns = [f'P_Growth_{col}' for col in df_potential_growth.columns]\n",
    "df_potential_growth.columns = [col.replace(\" \", \"_\") for col in df_potential_growth.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'occupe maintenant de transformer les index dans un format trimestriel, pour cela on va procéder à une interpolation polynomiale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assurer que l'index est bien au format datetime pour éviter les erreurs\n",
    "df_potential_growth.index = pd.to_datetime(df_potential_growth.index, format=\"%Y\")\n",
    "\n",
    "# Répéter chaque ligne 4 fois pour chaque trimestre\n",
    "df_quarterly = df_potential_growth.loc[df_potential_growth.index.repeat(4)].copy()\n",
    "\n",
    "# Générer les trimestres dynamiquement pour correspondre au bon nombre de lignes\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4'] * (len(df_potential_growth))\n",
    "df_quarterly['Quarter'] = quarters[:len(df_quarterly)] \n",
    "\n",
    "# Diviser toutes les colonnes numériques par 4\n",
    "for col in df_potential_growth.columns:\n",
    "    df_quarterly[col] = df_quarterly[col] / 4\n",
    "\n",
    "# Construire l'index trimestriel\n",
    "df_quarterly.index = [f\"{year.year}-{q}\" for year, q in zip(df_potential_growth.index.repeat(4), df_quarterly['Quarter'])]\n",
    "\n",
    "# Supprimer la colonne \"Quarter\" qui n'est plus nécessaire\n",
    "df_quarterly.drop(columns=['Quarter'], inplace=True)\n",
    "\n",
    "df_potential_growth = df_quarterly.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains when parsing with format \"%Y\": \"-Q1\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assurer que l'index est au format datetime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_potential_growth.index = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_potential_growth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convertir les années en format numérique pour l'interpolation\u001b[39;00m\n\u001b[32m      5\u001b[39m years = df_potential_growth.index.year\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1076\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1074\u001b[39m         result = _convert_and_box_cache(arg, cache_array, name=arg.name)\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m         result = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[32m   1078\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1079\u001b[39m         \u001b[38;5;66;03m# error: Argument 1 to \"_maybe_cache\" has incompatible type\u001b[39;00m\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# \"Union[float, str, datetime, List[Any], Tuple[Any, ...], ExtensionArray,\u001b[39;00m\n\u001b[32m   1081\u001b[39m         \u001b[38;5;66;03m# ndarray[Any, Any], Series]\"; expected \"Union[List[Any], Tuple[Any, ...],\u001b[39;00m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;66;03m# Union[Union[ExtensionArray, ndarray[Any, Any]], Index, Series], Series]\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:433\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    436\u001b[39m     arg,\n\u001b[32m    437\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    442\u001b[39m )\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:467\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    457\u001b[39m     arg,\n\u001b[32m    458\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    463\u001b[39m ) -> Index:\n\u001b[32m    464\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    469\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstrptime.pyx:587\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: unconverted data remains when parsing with format \"%Y\": \"-Q1\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Assurer que l'index est au format datetime\n",
    "df_potential_growth.index = pd.to_datetime(df_potential_growth.index, format=\"%Y\")\n",
    "\n",
    "# Convertir les années en format numérique pour l'interpolation\n",
    "years = df_potential_growth.index.year\n",
    "\n",
    "# Générer un nouvel index trimestriel (ex: 2000.0, 2000.25, 2000.5, 2000.75, ...)\n",
    "new_index = np.arange(years.min(), years.max() + 0.75, 0.25)\n",
    "\n",
    "# DataFrame final avec interpolations\n",
    "df_interpolated = pd.DataFrame(index=new_index)\n",
    "\n",
    "# Fonction pour calculer l'AIC\n",
    "def compute_aic(y_true, y_pred, k):\n",
    "    n = len(y_true)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    aic = n * np.log(mse) + 2 * k\n",
    "    return aic\n",
    "\n",
    "# Boucle sur chaque colonne du DataFrame\n",
    "best_degrees = {}  # Dictionnaire pour stocker le meilleur degré pour chaque colonne\n",
    "for col in df_potential_growth.columns:\n",
    "    values = df_potential_growth[col].dropna().values  # Supprimer les NaN\n",
    "    valid_years = df_potential_growth[col].dropna().index.year  # Années valides (sans NaN)\n",
    "    \n",
    "    if len(values) < 2:  # Vérifier qu'on a assez de points pour interpoler\n",
    "        print(f\"Pas assez de données pour interpoler {col}\")\n",
    "        df_interpolated[col] = np.nan\n",
    "        continue\n",
    "    \n",
    "    # Trouver le degré optimal du polynôme\n",
    "    best_degree = None\n",
    "    best_aic = np.inf\n",
    "    for deg in range(1, min(4, len(values))):  # Limiter le degré max à éviter les erreurs\n",
    "        try:\n",
    "            poly_fit = np.polyfit(valid_years, values, deg=deg)\n",
    "            poly_model = np.poly1d(poly_fit)\n",
    "            predictions = poly_model(valid_years)\n",
    "\n",
    "            if np.isnan(predictions).any():  # Vérifier si le polynôme donne NaN\n",
    "                continue\n",
    "\n",
    "            aic = compute_aic(values, predictions, deg + 1)  # k = deg + 1 paramètres du polynôme\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_degree = deg\n",
    "        except:\n",
    "            continue  # Si une erreur survient, on passe au degré suivant\n",
    "\n",
    "    best_degrees[col] = best_degree  # Stocker le meilleur degré trouvé\n",
    "    \n",
    "    # Appliquer l'interpolation polynomiale sur l'index trimestriel\n",
    "    if best_degree is not None:\n",
    "        best_poly_fit = np.polyfit(valid_years, values, best_degree)\n",
    "        best_poly_model = np.poly1d(best_poly_fit)\n",
    "        df_interpolated[col] = best_poly_model(new_index)\n",
    "    else:\n",
    "        df_interpolated[col] = np.nan  # Si pas de modèle valide, mettre NaN\n",
    "\n",
    "# Générer les labels trimestriels \"YYYY-Qx\"\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "quarter_labels = [f\"{int(year)}-{q}\" for year in new_index for q in quarters][:len(df_interpolated)]\n",
    "df_interpolated.index = quarter_labels  # Appliquer les labels trimestriels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_interpolated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_interpolated\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_interpolated' is not defined"
     ]
    }
   ],
   "source": [
    "df_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_potential_growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la base de données <a class=\"anchor\" id=\"partie3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que tous les dataframes sont construits pour les variables d'intérêts nous concaténons les dataframes pour en avoir un unique qui servira de base de données pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des DataFrames\n",
    "df_concat = pd.concat([df_GDP, df_CPI,df_LT_IR, df_ST_IR, df_working_hours, df_potential_growth], axis=1)\n",
    "\n",
    "# Réorganisation des colonnes pour que celles de chaque pays soient côte à côte\n",
    "ordered_columns = []\n",
    "\n",
    "# Modification la liste de base pour qu'elle corresponde au format de notre dataframe\n",
    "countries_selected.remove('Slovak Republic')\n",
    "countries_selected.remove('United Kingdom')\n",
    "countries_selected.remove('Czech Republic')\n",
    "countries_selected.append('United_Kingdom')\n",
    "\n",
    "# Tri\n",
    "for country in countries_selected:\n",
    "    ordered_columns.append(f'CPI_{country}')\n",
    "    ordered_columns.append(f'PIB_{country}')\n",
    "    ordered_columns.append(f'LT_IR_{country}')\n",
    "    ordered_columns.append(f'ST_IR_{country}')\n",
    "    ordered_columns.append(f'WH_{country}')\n",
    "    ordered_columns.append(f'P_Growth_{country}')\n",
    "\n",
    "df_final = df_concat[ordered_columns]\n",
    "\n",
    "# Exportation\n",
    "df_final.to_excel('base_de_données_v1.xlsx', index=True, engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
